test.connection.success=Test Connection Successfully
assign.menu.failed=Assign Menu Failed
ldap.user.autoload.forbaid=Auto-mapping LDAP users are not enabled, please contact your administrator to import
cluster.instance.recycle.success=Recycle Success
execute.failed=Execute Failed
ldap.user.duplicat=The ldap matches to multiple user data
git.branch.not.found=Git Branch Not Found
copy.success=Copy Successfully
user.superadmin.cannot.disable=User SuperAdmin Cannot Disable
user.superadmin.cannot.delete=User SuperAdmin Cannot Delete
ds.work.flow.not.save=Please Save Workflow First
schedule.status.unknown=Unknown Status: {0}
user.binding.role.delete.all=User Binding Role Delete All
modify.failed=Update Failed
git.build.success=Pre-update status success, start executing the build process
menu.has.child=Menu Has Child, Can Not Delete
tenant.already.exists=Tenant Already Exists
save.failed=Save Failed
assign.menu.success=Assign Menu Success
user.disabled.by.admin=The Current User is Disabled By Admin
select.menu=Please Select Menu
role.not.exist=Role Not Exists
delete.success=Delete Successfully
clear.success=Clear Successfully
move.success=Move Successfully
ldap.login.forbid=If the current user login mode is not LDAP, contact the administrator to modify it, or do not use LDAP to log in
request.params.not.valid.error=Request Parameter {0} Is Not Valid
change.password.failed=Change Password Failed
menu.name.exist=Menu Already Exists
ds.task.type.not.support=DolphinScheduler Type Is [{}] Not Support, Not DINKY Type
datasource.connect.normal=DataSource Connect Normal
restart.success=Restart Successfully
test.msg.job.log.url=Click to view the exception log for this task
user.assign.role.success=User Assign Role Success
global.params.check.error.value=Field: {0}, Illegal Value: {1}
change.password.success=Change Password Success
user.not.exist=User Not Exist
refresh.success=Refresh Successfully
ds.get.node.list.error=Get Node List Error
ldap.default.tenant.nofound=The LDAP default tenant does not exist
copy.failed=Copy Failed
folder.not.empty=Folder Not Empty, Can Not Delete
be.replaced=token has been pushed offline
datasource.connect.success=DataSource Connect Success
sign.out.success=Sign Out Successfully
added.success=Added Successfully
tenant.binding.user=Tenant Binding User , Can Not Delete
send.test.failed=Test Msg Send Fail
delete.failed=Delete Failed
role.binding.user=Role Already Binding User , Can Not Delete
not.token=Can Not Read Token
execute.success=Execute Successfully
debug.success=Debug Successfully
debug.failed=Debug Failed
publish.success=Publish Successfully
publish.failed=Publish Failed
offline.success=Offline Successfully
offline.failed=Offline Failed
version.rollback.success=Version Rollback Successfully
version.rollback.failed=Version Rollback Failed
token.freezed=token has been frozen
menu.has.assign=Menu Has Assign , Can Not Delete
datasource.status.refresh.success=DataSource Status Refresh Success
user.not.login=User is Not Login
tenant.assign.user.failed=Tenant Assign User Failed
stop.success=Stop Successfully
move.failed=Move Failed
get.tenant.failed=Get Tenant Info Failed
send.test.success=Test Msg Send Success
login.success=Login Successfully
login.password.not.null=Login Password Not Null
unknown.error=Unknown Error: {0}
stop.failed=Stop Failed
role.name.exist=Role Already Exists
ldap.filter.incorrect=If the user filter rule cannot be empty, enter the relevant configuration
tenant.assign.user.success=Tenant Assign User Success
ds.add.work.flow.definition.success=Add Workflow Definition Success
expired.token=Expired Token
refresh.failed=Refresh Failed
operate.success=Operate Successfully
git.project.not.found=Git Project Not Found
cluster.instance.heartbeat.success=Cluster Instance Heartbeat Success
ldap.no.user.found=The LDAP connection was successful, but it did not match to any users
login.failure=User Login Failure
request.params.error=Request Parameter Error
user.not.binding.tenant=User Not Binding Tenant
user.assign.role.failed=User Assign Role Failed
rename.failed=Rename Failed
test.msg.job.name=Job of Test
tenant.binding.user.delete.all=Tenant Binding User Delete All
menu.not.exist=Menu Not Exists
test.msg.job.name.title=Task
ds.task.not.exist=Task Not Exist
global.params.check.error=Field: {0}, {1}
test.msg.title=Real Time alarm mertics
user.name.passwd.error=UserName Or Password Not Correct
no.prefix=The token was not submitted according to the specified prefix
query.success=Query Successfully
ds.work.flow.definition.not.exist=Workflow Definition Not Exist, You Can Add Workflow Definition
ds.work.flow.definition.process.update=Workflow Definition [{}] Update, TaskCode: [{}], Parameter 1: [{}], Parameter 2: [{}]
tenant.name.exist=Tenant Already Exists
failed=Failed
added.failed=Added Failed
task.not.exist=Task Not Exist
task.is.online= Task is online, modification is prohibited
task.is.existed=Task is existed
cluster.instance.deploy=Deploy Success
clear.failed=Clear Failed
rename.success=Rename Successfully
job.release.disabled.update=Assignment Has Been Published, Modification is Prohibited
success=Successfully
tenant.not.exist=Tenant Not Exist
user.already.exists=User Already Exists
git.building=Git Building
ds.work.flow.definition.task.name.exist=Workflow Definition [{}] Already Exists Task Definition [{}] , Will Update Operation
role.already.exists=Role Already Exists
internal.server.error.args=Internal Server Error: {0}
kick.out=token has been kicked offline
restart.failed=Restart Failed
invalid.token=Invalid Token
datasource.not.exist=DataSource Not Exist
datasource.clear.cache.success=DataSource Clear Cache Success
datasource.exist.relationship=DataSource Already Exists Relationship, Can Not Delete
tenant.admin.already.exists=Tenant Admin Already Exists, Only One Tenant Admin
ds.work.flow.definition.online=Workflow Definition [{}] Has Been Online
test.msg.job.url=Jump to the task
savepoint.is.null=Savepoint Is Null
git.sort.success=Git Sort Success
ds.add.task.definition.success=Add Task Definition Success
alert.group.exist=Alert Group Already Exists
alert.group.exist.relationship=Alert Group Already Exists Relationship, Can Not Delete
alert.template.exist.relationship=Alert Template Already Exists Relationship, Can Not Delete
git.sort.failed=Git Sort Failed
query.failed=Query Failed
save.success=Save Successfully
cluster.instance.kill=Kill Success
cluster.not.exist=Cluster Not Exist
cluster.instance.exist.relationship=Cluster Instance Already Exists Relationship, Can Not Delete
cluster.config.exist.relationship=Cluster Config Already Exists Relationship, Can Not Delete
udf.template.exist.relationship=Udf Template Already Exists Relationship, Can Not Delete
operate.failed=Operate Failed
test.connection.failed=Test Connection Failed
switching.tenant.success=Select Tenant Success
tenant.name.not.exist=Tenant Not Exists
job.instance.not.exist=Job Instance Not Exist
modify.success=Update Successfully
user.old.password.incorrect=User Old Password Incorrect
ldap.user.incorrect=The LDAP user name (DN) Incorrect
role.binding.row.permission=Role Already Binding Row Permission , Can Not Delete

# dinky-admin
unknown.i18n=Unknown i18n information, please check. . .

file.upload.failed=File upload failed, reason: {0}

# dinky-alert
alert.rule.jobFail=Job Failure
alert.rule.getJobInfoFail=Get Job Info Fail
alert.rule.jobRestart=Job Restart
alert.rule.checkpointFail=Checkpoint Failure
alert.rule.jobRunException=Job Run Exception
alert.rule.checkpointTimeout=Checkpoint Timeout

daemon.task.config.not.exist=the thread task configuration can not be empty
daemon.task.not.support=threaded task types are notsupported


# system config
sys.flink.settings.useRestAPI=Use Rest API
sys.flink.settings.useRestAPI.note=Whether to use RestAPI when operating and maintaining Flink tasks
sys.flink.settings.sqlSeparator=SQL Separator
sys.flink.settings.sqlSeparator.note=please note: The default delimiter is ';', if your SQL contains ';', please modify this configuration to other characters, for example: ';\\n', please ignore the single quotes!!!!
sys.flink.settings.jobIdWait=Job submission waiting time
sys.flink.settings.jobIdWait.note=Maximum waiting time (seconds) for obtaining Job ID when submitting Application or PerJob tasks
sys.maven.settings.settingsFilePath=Maven configuration file path
sys.maven.settings.settingsFilePath.note=Maven configuration file path, eg: /opt/maven-3.9.2/conf/settings.xml, please note: By default, the MAVEN_HOME environment variables of the host will be obtained, so there is no need to fill in here. If there is no configuration, please fill in the absolute path.
sys.maven.settings.repository=Maven warehouse address
sys.maven.settings.repository.note=Maven warehouse address
sys.maven.settings.repositoryUser=Maven warehouse user name
sys.maven.settings.repositoryUser.note=Maven private server authentication user name, if you need to configure Maven private server warehouse authentication information, please fill in this
sys.maven.settings.repositoryPassword=Maven warehouse password
sys.maven.settings.repositoryPassword.note=Maven private server authentication password, if you need to configure Maven private server warehouse authentication information, please fill in this item
sys.env.settings.pythonHome=Python Env variables
sys.env.settings.pythonHome.note=Python environment variables, used to submit Python tasks and build Python Udf
sys.env.settings.dinkyAddr=Dinky Address
sys.env.settings.dinkyAddr.note=The address must be the same as the address configured in the Dinky Application background url
sys.env.settings.maxRetainDays=Job history max retained days
sys.env.settings.maxRetainDays.note=The maximum number of days for the history of submitted jobs and auto-registered cluster records to be retained will be automatically deleted when they expire
sys.env.settings.maxRetainCount=Job history max retained counts
sys.env.settings.maxRetainCount.note=The maximum number of submitted job histories and auto-registered cluster records will not be deleted if they are less than that number, even if the retention days have passed

sys.dolphinscheduler.settings.enable=Whether to enable DolphinScheduler
sys.dolphinscheduler.settings.enable.note=Whether to enable DolphinScheduler. Only after enabling it can you use the related functions of DolphinScheduler. Please fill in the following configuration items first, and then enable this configuration after completion. Also: Please ensure that the related configurations of DolphinScheduler are correct.
sys.dolphinscheduler.settings.url=DolphinScheduler address
sys.dolphinscheduler.settings.url.note=The address must be consistent with the address configured in the DolphinScheduler background, eg: http://127.0.0.1:12345/dolphinscheduler
sys.dolphinscheduler.settings.token=DolphinScheduler Token
sys.dolphinscheduler.settings.token.note=DolphinScheduler's Token, please create a token in DolphinScheduler's Security Center->Token Management, and fill in the configuration
sys.dolphinscheduler.settings.projectName=DolphinScheduler project name
sys.dolphinscheduler.settings.projectName.note=The project name specified in DolphinScheduler, case insensitive
sys.ldap.settings.url=ldap address of service
sys.ldap.settings.url.note=ldap address of service, eg: ldap://192.168.111.1:389
sys.ldap.settings.userDn=Login User name (DN)
sys.ldap.settings.userDn.note=User name for connecting to the ldap service, or the administrator DN
sys.ldap.settings.userPassword=login password
sys.ldap.settings.userPassword.note=Password used to connect to the ldap service
sys.ldap.settings.timeLimit=Connection Timeout
sys.ldap.settings.timeLimit.note=The maximum time to connect to the ldap service is disconnected
sys.ldap.settings.baseDn=BaseDn
sys.ldap.settings.baseDn.note=Dinky will conduct a user search on this base dn,eg: ou=users,dc=dinky,dc=com
sys.ldap.settings.filter=User filtering rules
sys.ldap.settings.filter.note=User filtering by using the filter syntax of the ldap, eg
sys.ldap.settings.autoload=User atically map users when logging in
sys.ldap.settings.autoload.note=When turned on, when a user logs in with LDAP, if there is no corresponding Dinky user mapping, the LDAP information is automatically pulled to create a Dinky user mapping to it. If this feature is closed, you will not be able to log in for unimported LDAP users
sys.ldap.settings.defaultTeant=The LDAP imports the default tenant code
sys.ldap.settings.defaultTeant.note=After opening the automatic import of users, the new user login needs a default tenant code, otherwise it cannot log in
sys.ldap.settings.castUsername=The LDAP user-name segment
sys.ldap.settings.castUsername.note=It is necessary to fill in the attribute field of the user in an LDAP to correspond to the Dinky user. Generally, cn or uid indicates the unique identity of the user
sys.ldap.settings.castNickname=The LDAP nickname field
sys.ldap.settings.castNickname.note=Need to fill in the attribute field of the user in an LDAP to correspond to the Dinky nickname, must be filled in, generally selected as sn or other identification, not required unique
sys.ldap.settings.enable=Whether to enable the ldap
sys.ldap.settings.enable.note=Turn the LDAP login function on
sys.metrics.settings.sys.enable=Dinky JVM Monitor switch
sys.metrics.settings.sys.enable.note=This switch is related to Dinky JVM Monitor, which determines the display of Dinky Server in the monitoring page and the collection of JVM Metrics
sys.metrics.settings.sys.gatherTiming=Dinky JVM Metrics collection time granularity
sys.metrics.settings.sys.gatherTiming.note=Dinky JVM Metrics collection time granularity, timed task interval trigger
sys.metrics.settings.flink.gatherTiming=Flink Metrics collection time granularity
sys.metrics.settings.flink.gatherTiming.note=Flink Metrics collection time granularity, scheduled task interval trigger
sys.metrics.settings.flink.gatherTimeout=Flink Metrics collection time granularity, scheduled task interval trigger
sys.metrics.settings.flink.gatherTimeout.note=Flink Metrics collection timeout period, scheduled task interval trigger (this configuration item should be smaller than Flink Metrics collection time granularity)
sys.resource.settings.base.enable=Whether to enable Resource
sys.resource.settings.base.enable.note=Enable resource management function. If you switch storage mode, you need to turn off this switch. After the relevant configuration is completed, turn it on again.
sys.resource.settings.base.upload.base.path=Root path of the upload directory
sys.resource.settings.base.upload.base.path.note=Resources are stored on the HDFS/OSS(S3) path. Resource files will be stored in this base path. Configure it by yourself. Please ensure that the directory exists on the relevant storage system and has read capabilities. Write permission. recommend
sys.resource.settings.base.model=Storage model
sys.resource.settings.base.model.note=Supports HDFS and S3(Minio,OSS,COS ant etc...), it will take effect after switching the option, and resource files will be migrated at the same time.
sys.resource.settings.oss.endpoint=URL of the object storage service(Endpoint)
sys.resource.settings.oss.endpoint.note=For example: https://oss-cn-hangzhou.aliyuncs.com
sys.resource.settings.oss.accessKey=Access key
sys.resource.settings.oss.accessKey.note=Access key is like a user ID that uniquely identifies your account
sys.resource.settings.oss.secretKey=Secret key
sys.resource.settings.oss.secretKey.note=Secret key is the password of your account
sys.resource.settings.oss.bucketName=Bucket name
sys.resource.settings.oss.bucketName.note=Default bucket name
sys.resource.settings.oss.region=region
sys.resource.settings.oss.region.note=region
sys.resource.settings.oss.path.style.access=Path Style
sys.resource.settings.oss.path.style.access.note=Whether to enable Path Style, Different providers (such as Alibaba Cloud OSS and Tencent Cloud COS) have different support conditions, please read the description of the provider

sys.resource.settings.hdfs.root.user=HDFS operation user name
sys.resource.settings.hdfs.root.user.note=HDFS operation user name
sys.resource.settings.hdfs.fs.defaultFS=HDFS defaultFS
sys.resource.settings.hdfs.fs.defaultFS.note=fs.defaultFS configuration items, such as remote: hdfs://localhost:9000, local: file:///


#Dinky Gateway
gateway.kubernetes.test.failed= failed to test the Flink configuration:

task.status.is.not.done=The current job status is not stopped, please stop after operation
task.sql.explain.failed=SQL parsing failed, please check the SQL statement
task.update.failed=Task Update failed

# process
process.submit.submitTask= Submit the job
process.submit.checkSql=Check job
process.submit.execute = execute the job
process.submit.buildConfig=Build configuration information
process.submit.execute.commSql=excute commonSql
process.submit.execute.flinkSql=excute flinkSql
process.register.exits=The current task is executing, do not submit it repeatedly, if you have any questions, please go to the configuration center to view
# resource
resource.root.dir.not.allow.delete=The root directory is not allowed to be deleted
resource.dir.or.file.not.exist=The directory or file does not exist